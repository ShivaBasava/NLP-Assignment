{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           id                   text  anger  fear  joy  \\\n",
      "1303  eng_train_track_a_01304  &lt;/crazy-nutter&gt;      0     1    0   \n",
      "\n",
      "      sadness  surprise  \n",
      "1303        0         1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def preprocess_track_a(df):\n",
    "    \"\"\"\n",
    "    Preprocesses the Track-A dataset:\n",
    "    - Lowercases text\n",
    "    - Removes punctuation (except intra-word apostrophes)\n",
    "    - Removes extra whitespace\n",
    "    - Ensures label columns are integers (0/1)\n",
    "    Returns a cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # label columns as per given dataset\n",
    "    label_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "    \n",
    "    # Lowercase and clean text\n",
    "    def clean_text(text):\n",
    "        if pd.isnull(text):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        # to retain alphanum\n",
    "        text = re.sub(r\"[^a-z0-9\\s']\", ' ', text)\n",
    "        # to normalize whitespace\n",
    "        text = re.sub(r\"\\s+\", ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    # Initially, cleaning only the 'text'\n",
    "    df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "    \n",
    "    # then, to ensure labels are integers (0/1)\n",
    "    for col in label_cols:\n",
    "        df[col] = df[col].fillna(0).astype(int)\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # location to the track-a.csv\n",
    "    df = pd.read_csv('track-a.csv')\n",
    "    # print(df.head())\n",
    "    print(df.loc[df['id'] == \"eng_train_track_a_01304\"])\n",
    "    # df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id                                               text  \\\n",
      "0  eng_train_track_a_00001                         colorado middle of nowhere   \n",
      "1  eng_train_track_a_00002  this involved swimming a pretty large lake tha...   \n",
      "2  eng_train_track_a_00003         it was one of my most shameful experiences   \n",
      "3  eng_train_track_a_00004  after all i had vegetables coming out my ears ...   \n",
      "4  eng_train_track_a_00005                         then the screaming started   \n",
      "\n",
      "   anger  fear  joy  sadness  surprise  \n",
      "0      0     1    0        0         1  \n",
      "1      0     1    0        0         0  \n",
      "2      0     1    0        1         0  \n",
      "3      0     0    0        0         0  \n",
      "4      0     1    0        1         1  \n",
      "                           id                text  anger  fear  joy  sadness  \\\n",
      "1303  eng_train_track_a_01304  lt crazy nutter gt      0     1    0        0   \n",
      "\n",
      "      surprise  \n",
      "1303         1  \n"
     ]
    }
   ],
   "source": [
    "# df_clean = preprocess_track_a(df)\n",
    "# # print(df_clean.head())\n",
    "\n",
    "# df_clean.head()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # location to the track-a.csv\n",
    "    df = pd.read_csv('track-a.csv')\n",
    "    df_clean = preprocess_track_a(df)\n",
    "\n",
    "    print(df_clean.head())\n",
    "\n",
    "    print(df.loc[df['id'] == \"eng_train_track_a_01304\"])\n",
    "    # df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from emot.emo_unicode import EMOTICONS_EMO  # For emoji handling (install: pip install emot)\n",
    "\n",
    "# Initialize spaCy without parser/ner for efficiency\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess_text(\n",
    "    text, \n",
    "    custom_emotion_map=None, \n",
    "    remove_stopwords=False, \n",
    "    lemmatize=True,\n",
    "    preserve_punct=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic text cleaner for emotion detection.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw input text.\n",
    "        custom_emotion_map (dict): Optional {symbol: replacement} for emotional cues.\n",
    "        remove_stopwords (bool): Remove stopwords if True.\n",
    "        lemmatize (bool): Lemmatize tokens if True.\n",
    "        preserve_punct (bool): Keep [!?] if True (critical for emotion detection).\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML/XML tags (handles malformed tags)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 2. Dynamic emotion symbol handling\n",
    "    emotion_map = {\n",
    "        **EMOTICONS_EMO,  # Auto-loads common emojis/emoticons (e.g., <3 â†’ 'love')\n",
    "        **{k: f' {v} ' for k, v in (custom_emotion_map or {}).items()}  # User overrides\n",
    "    }\n",
    "    \n",
    "    # Replace symbols/emojis (regex-safe)\n",
    "    for symbol, replacement in emotion_map.items():\n",
    "        text = re.sub(re.escape(symbol), replacement, text)\n",
    "    \n",
    "    # 3. Preserve or strip punctuation\n",
    "    punct_pattern = r'[^\\w\\s]' if not preserve_punct else r'[^\\w\\s!?]'\n",
    "    text = re.sub(punct_pattern, ' ', text)\n",
    "    \n",
    "    # 4. Normalize whitespace and lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    \n",
    "    # 5. Advanced tokenization (optional)\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ if lemmatize and token.lemma_ != '-PRON-' else token.text\n",
    "        for token in doc\n",
    "        if not (remove_stopwords and token.is_stop)\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example usage with custom symbols\n",
    "custom_map = {\n",
    "    '!!!': ' high_intensity ',  # Custom intensity marker\n",
    "    '?': ' uncertainty '       # Label questions\n",
    "}\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(\n",
    "    preprocess_text, \n",
    "    custom_emotion_map=custom_map,\n",
    "    preserve_punct=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
